# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_data-core.ipynb (unless otherwise specified).

__all__ = ['HF_TokenizerTransform', 'HF_BaseInput', 'HF_BatchTransform', 'pad_hf_inputs', 'HF_TextBlock']

# Cell
from functools import reduce

import torch, nlp
from transformers import *
from fastai.text.all import *

from ..utils import *

# Cell
class HF_TokenizerTransform(ItemTransform):
    """huggingface friendly tokenization transform."""
    def __init__(self, hf_arch, hf_tokenizer,
                 max_length=None, padding=True, truncation=True, is_pretokenized=False, **kwargs):

        # gpt2, roberta, bart (and maybe others) tokenizers require a prefix space
        if (hasattr(hf_tokenizer, 'add_prefix_space')): kwargs['add_prefix_space'] = True

        store_attr(self=self, names='hf_arch, hf_tokenizer, is_pretokenized, max_length, padding, truncation')
        store_attr(self=self, names='kwargs')

    def encodes(self, inp):
        """Supports passing in one or two input sequences, or a list[str] (the later is common for token
        classification tasks where you should also set `is_pretokenized=True`).
        Returns all the tensors for the input sequence(s) in a dictionary."""
        inps = [inp, None] if (isinstance(inp, str) or self.is_pretokenized) else inp

        res = self.hf_tokenizer(inps[0], inps[1],
                                max_length=self.max_length,
                                padding=self.padding,
                                truncation=self.truncation,
                                is_pretokenized=self.is_pretokenized,
                                return_tensors='pt',
                                **self.kwargs)

        for k in res.keys(): res[k] = res[k].squeeze(0)
        return res

    def decodes(self, encoded_inp):
        """Returns the first item of the list `encoded_inp`; this should be the 'input_ids'."""
        input_ids = filter(lambda el: el != self.hf_tokenizer.pad_token_id, encoded_inp[0].cpu().numpy())
        decoded_input = self.hf_tokenizer.decode(input_ids, skip_special_tokens=True)
        return TitledStr(decoded_input)


# Cell
class HF_BaseInput(list): pass

# Cell
class HF_BatchTransform(Transform):
    """Handles everything you need to assemble a mini-batch of inputs and targets, as well as decode
    HF_TokenizerTransform inputs
    """
    def __init__(self, hf_arch, hf_tokenizer, hf_input_return_type=HF_BaseInput, **kwargs):
        store_attr(self=self, names='hf_arch, hf_tokenizer, hf_input_return_type, kwargs')

    def encodes(self, samples): return samples

    def decodes(self, encoded_samples):
        if (isinstance(encoded_samples, dict)): return self.hf_input_return_type([encoded_samples['input_ids']])
        return encoded_samples

# Cell
def pad_hf_inputs(samples, arch, hf_input_idxs=[0], pad_idx=0, pad_first=False):
    """
    Add this to your batch transforms if you are using dynamic padding with `HF_TokenizerTransform`
    (e.g., padding is set to anything except 'max_length') to ensure all HF tensors are sized to longest input
    in the batch.

    Note: This is automatically included as necessary by `HF_TextBlock`
    """
    for hf_input_idx in hf_input_idxs:
        if (hf_input_idx >= len(samples[0])): continue

        inp_keys = samples[0][hf_input_idx].keys()
        max_len = np.max([len(s[hf_input_idx]["input_ids"]) for s in samples])

        for idx, sample in enumerate(samples):
            for key in inp_keys:
                if (key == 'input_ids'): tok_id = pad_idx
                elif (key == 'special_tokens_mask'): tok_id=1
                elif (arch == 'xlnet' and key == 'token_type_ids'): tok_id = 3
                else: tok_id = 0

                if (pad_first):
                    new_val = torch.cat((sample[hf_input_idx][key].new_full((max_len,), tok_id),
                                         sample[hf_input_idx][key]), dim=0)[-max_len:]
                else:
                    new_val = torch.cat((sample[hf_input_idx][key],
                                         sample[hf_input_idx][key].new_full((max_len,), tok_id)), dim=0)[:max_len]

                samples[idx][hf_input_idx][key] = new_val[:max_len]

    return samples

# Cell
class HF_TextBlock(TransformBlock):
    def __init__(self, hf_arch, hf_tokenizer,
                 hf_tok_tfm=None, max_length=512, padding=True, truncation=True, is_pretokenized=False,
                 hf_batch_tfm=None, hf_input_return_type=HF_BaseInput, hf_input_idxs=[0],
                 dl_type=SortedDL, tok_kwargs={}, batch_kwargs={}, **kwargs):

        if (hf_tok_tfm is None):
            hf_tok_tfm = HF_TokenizerTransform(hf_arch, hf_tokenizer, max_length,
                                               padding, truncation, is_pretokenized, **tok_kwargs)

        if (hf_batch_tfm is None):
            hf_batch_tfm = HF_BatchTransform(hf_arch, hf_tokenizer, hf_input_return_type, **batch_kwargs)

        pad_fn = noop
        if padding != 'max_length':
            pad_fn = partial(pad_hf_inputs,
                             arch=hf_arch,
                             hf_input_idxs=hf_input_idxs,
                             pad_idx=hf_tokenizer.pad_token_id,
                             pad_first=hf_tokenizer.padding_side=='left')

        return super().__init__(type_tfms=hf_tok_tfm,
                                dl_type=dl_type,
                                dls_kwargs={ 'before_batch': [pad_fn, hf_batch_tfm] })

# Cell
@typedispatch
def show_batch(x:HF_BaseInput, y, samples, dataloaders=None, ctxs=None, max_n=6, **kwargs):
    if ctxs is None: ctxs = get_empty_df(min(len(samples), max_n))
    ctxs = show_batch[object](x, y, samples, max_n=max_n, ctxs=ctxs, **kwargs)

    display_df(pd.DataFrame(ctxs))
    return ctxs