Metadata-Version: 2.1
Name: sqlicrawler
Version: 0.1.3
Summary: SQLi Crawler with JavaScript support.
Author: Sergey M
Author-email: tz4678@gmail.com
Requires-Python: >=3.8,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Topic :: Internet
Classifier: Topic :: Security
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Utilities
Requires-Dist: aiodns (>=2.0.0,<3.0.0)
Requires-Dist: aiohttp (>=3.6.2,<4.0.0)
Requires-Dist: aiohttp_socks (>=0.5.3,<0.6.0)
Requires-Dist: click (>=7.1.2,<8.0.0)
Requires-Dist: pyppeteer (>=0.2.2,<0.3.0)
Requires-Dist: ujson (>=3.1.0,<4.0.0)
Description-Content-Type: text/markdown

# SQLiCrawler

**sqlicrawler** - это утилита, созданная для автоматизации поиска sql-инъекций. Она запускает экземпляр браузера Chromium Headless и посещает ссылки на сайте, выполняя скрипты на JavaScript и отправляя формы. Это моя 100500 реализация сканнера уязвимостей.

С целью ускорения загрузки страниц загрузка стилей и изображений блокируется.

![image](https://user-images.githubusercontent.com/12753171/91443290-cd3a6880-e87b-11ea-8ac1-703880a5ebee.png)

Такой паук "видит" страницу выше:

![image](https://user-images.githubusercontent.com/12753171/91443491-168ab800-e87c-11ea-8faf-1f0da95eb987.png)

## Usage

```zsh
# install the package into a virtual environment and create an executable in the ~/.local/bin directory
$ pipx install sqlicrawler
$ sqlicrawler --help
```

В качестве значения флага `-i` используется путь до файла, в котором содержится список ссылок (каждая с новой строки) на сайты для сканирования. С помощью флага `-o` задается имя файла в котором будут храниться результаты сканирования. этот файл имеет формат JSONL. Каждая его строка представляет собой сериализованный в JSON объект. Для парсинга файлов такого типа рекомендуется использовать утилиту **jq**.

Если Вы хотите заблокировать нежелательные запросы к определенным сайтам, например, к скриптам, собирающим статистику поситетилей сайта, то нужно создать файл `~/.config/sqlicrawler/blacklist.txt`, содержащий шаблоны запрещенных ссылок. Часть имени, содержащую произвольное количество символов можно заменить на `*`.

Сессионная информация браузера лежит в `~/.config/sqlicrawler/chrome_profile`.

## Development

```zsh
$ poetry run pytest -s tests
```

